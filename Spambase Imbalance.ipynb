{"cells":[{"cell_type":"markdown","metadata":{"id":"1YZ6b0u2K--g"},"source":["#### **1. Import and Load Dataset**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"jnwmyNxkK--i","executionInfo":{"status":"error","timestamp":1717603952369,"user_tz":-420,"elapsed":8460,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}},"outputId":"610024d0-a3cc-4544-ba5f-c8ecc64e4aec"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'ucimlrepo'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e0ffb9001e91>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m  \u001b[0;31m# For building logistic regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m  \u001b[0;31m# For evaluating model performance using various metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mucimlrepo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_ucirepo\u001b[0m  \u001b[0;31m# For fetching datasets from UCI Machine Learning Repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m  \u001b[0;31m# For building Support Vector Machine models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m  \u001b[0;31m# For building Random Forest Classifier models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ucimlrepo'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Importing necessary libraries\n","import xgboost as xgb  # For fast and effective gradient boosting, excelling in classification and regression tasks, especially with large datasets.\n","import pandas as pd  # For data manipulation and analysis\n","import matplotlib.pyplot as plt  # For creating plots and visualizations\n","import numpy as np  # For numerical computations\n","import seaborn as sns  # For statistical data visualization\n","import tensorflow as tf  # For building and training neural networks\n","from sklearn.impute import KNNImputer  # For imputing missing values using KNN algorithm\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV  # For splitting data into train and test sets, and for hyperparameter tuning\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder  # For encoding categorical variables, scaling numerical features to a specified range, and imputing missing values\n","from sklearn.linear_model import LogisticRegression  # For building logistic regression model\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report  # For evaluating model performance using various metrics\n","from ucimlrepo import fetch_ucirepo  # For fetching datasets from UCI Machine Learning Repository\n","from sklearn.svm import SVC  # For building Support Vector Machine models\n","from sklearn.ensemble import RandomForestClassifier  # For building Random Forest Classifier models\n","from sklearn.neural_network import MLPClassifier  # For building Multilayer Perceptron Classifier models\n","from tensorflow.keras.models import Sequential  # For building sequential neural network models\n","from tensorflow.keras.layers import Dense  # For adding dense layers to neural network models\n","from scikeras.wrappers import KerasClassifier  # For using Keras models with scikit-learn interface\n","from sklearn.compose import ColumnTransformer  # For applying transformations to different columns in the dataset\n","from scipy.stats import randint  # For generating random integer values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxFz9pptK--j","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":10,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["from ucimlrepo import fetch_ucirepo  # For fetching datasets from the UCI Machine Learning Repository\n","\n","# Fetch dataset\n","spambase = fetch_ucirepo(id=94)  # Fetch the Spambase dataset with ID 94\n","\n","# Data (as pandas dataframes)\n","X = spambase.data.features  # Features of the Spambase dataset\n","y = spambase.data.targets  # Target labels of the Spambase dataset\n","\n","# Metadata\n","print(spambase.metadata)  # Print metadata information about the Spambase dataset\n","\n","# Variable information\n","print(spambase.variables)  # Print information about the variables/features in the Spambase dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxfNmkyPK--k","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":10,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["df = pd.concat([X, y], axis=1, join='inner')  # Concatenate features and target labels along columns with inner join\n","display(df)  # Display the concatenated DataFrame"]},{"cell_type":"markdown","metadata":{"id":"f3qJrVgIK--k"},"source":["#### **2. EDA**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPiTIZd_K--k","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Viewing the first few rows of the dataframe\n","print(df.head())  # Display the first few rows of the DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBgIG1WFK--k","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["df.isna().sum()  # Check the number of missing values for each column in the DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmQYkVmUK--l","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Viewing general information about the dataframe\n","print(df.info())  # Print general information about the DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBkZMMGTK--l","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Viewing descriptive statistics of the dataframe\n","print(df.describe(include='all'))  # Print descriptive statistics of the DataFrame, including all columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUPnUoNJK--l","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Selecting numerical columns\n","numerical = df.select_dtypes(include=['float', 'int']).columns\n","# Identify and store the column names of numerical features in the DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syDJNQlQK--l","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Extracting only the numerical columns from the DataFrame\n","numerical_df = df[numerical]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgbrBeH5K--l","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Visualizing the distribution of each feature using scatter plots\n","for feature in numerical_df.columns:\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(range(len(df)), df[feature], alpha=0.7)  # Scatter plot of the feature\n","    plt.title(f'Distribution of {feature}')  # Title of the plot\n","    plt.xlabel('Index')  # X-axis label\n","    plt.ylabel(feature)  # Y-axis label\n","    plt.show()  # Display the plot"]},{"cell_type":"markdown","metadata":{"id":"meloHPByK--m"},"source":["#### **3. Feature Engineering**"]},{"cell_type":"markdown","metadata":{"id":"hK5U1rSwK--m"},"source":["**Prepare the features set and target variable**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PxUS3uNK--m","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Select rows where the \"Class\" column contains NaN values\n","df[df[\"Class\"].isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ozQxFSRK--m","executionInfo":{"status":"aborted","timestamp":1717603952370,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate the proportion of missing values for each numerical column in the DataFrame\n","df[numerical].isna().mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rk6NhUPHK--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# check data types in DataFrame\n","df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBTPfly7K--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Generate descriptive statistics for the numerical columns in the DataFrame\n","df[numerical].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CxDckjQK--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Extracting only the numerical columns from the DataFrame and storing it in a new DataFrame\n","df_numeric = df[numerical]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkYNbI35K--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Ensure the presence of the 'Class' column before proceeding\n","if 'Class' not in df.columns:\n","    raise KeyError(\"The 'Class' column is not present in the DataFrame\")\n","\n","# Select rows where the \"Class\" column contains NaN values\n","class_nan_rows = df[df[\"Class\"].isna()]\n","print(\"Class NaN Rows:\\n\", class_nan_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFlZWKHzK--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate the proportion of missing values for each numerical column in the DataFrame\n","numerical = df.select_dtypes(include=[np.number]).columns.tolist()\n","missing_proportion = df[numerical].isna().mean()\n","print(\"Missing Proportion:\\n\", missing_proportion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjmroCYQK--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Generate descriptive statistics for the numerical columns in the DataFrame\n","print(\"Descriptive Statistics:\\n\", df[numerical].describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bL4TARSK--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Extracting only the numerical columns from the DataFrame and storing it in a new DataFrame\n","df_numeric = df[numerical].copy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvBkXcWAK--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Function to calculate outliers' bounds using Interquartile Range (IQR)\n","def calculate_outliers_bound(df: pd.DataFrame, col: str) -> (float, float):\n","    \"\"\"\n","    Calculate the lower and upper bounds for outliers detection using Interquartile Range (IQR) method.\n","\n","    Parameters:\n","        df (pd.DataFrame): The DataFrame containing the numerical column.\n","        col (str): The name of the numerical column for which outliers bounds are calculated.\n","\n","    Returns:\n","        (float, float): A tuple containing the lower and upper bounds for outliers detection.\n","    \"\"\"\n","    q1 = df[col].quantile(0.25)\n","    q3 = df[col].quantile(0.75)\n","    iqr = q3 - q1\n","    lower_bound = q1 - 1.5 * iqr\n","    upper_bound = q3 + 1.5 * iqr\n","    return (lower_bound, upper_bound)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j07CS1c8K--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Detect outliers using the Interquartile Range (IQR) method and replace them with NaN values\n","outliers_bound = {col: calculate_outliers_bound(df_numeric, col) for col in numerical}\n","for col in numerical:\n","    lower_bound, upper_bound = outliers_bound[col]\n","    df_numeric.loc[(df_numeric[col] < lower_bound) | (df_numeric[col] > upper_bound), col] = np.nan\n","\n","# Check the number of missing values for each numerical column after outlier detection\n","print(\"Missing values before imputation:\\n\", df_numeric.isna().sum())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO2BAdDYK--m","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Using KNNImputer to fill missing values\n","imputer = KNNImputer()\n","df_imputed_array = imputer.fit_transform(df_numeric)\n","df_imputed = pd.DataFrame(df_imputed_array, columns=numerical)\n","\n","# Add the 'Class' column back to the imputed DataFrame\n","df_imputed['Class'] = df['Class'].values\n","\n","# Checking the distribution of the target variable\n","frequencies = df_imputed['Class'].value_counts()\n","proportions = df_imputed['Class'].value_counts(normalize=True) * 100\n","print(frequencies)\n","print(proportions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxObtwsXK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Output the engineered DataFrame for further processing\n","print(\"Here's the engineered DataFrame:\\n\", df_imputed.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZyjWwDJK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Plotting the distribution of each numerical feature\n","for feature in numerical:\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(df_imputed.index, df_imputed[feature], alpha=0.7)\n","    plt.title(f'Distribution of {feature}')\n","    plt.xlabel('Index')\n","    plt.ylabel(feature)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErObKkGVK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Assigning the target variable\n","y = df_imputed[\"Class\"]\n","\n","# Removing the target column from the feature dataset\n","X = df_imputed.drop([\"Class\"], axis=1)\n","\n","# Counting the frequency and proportion of each category\n","frequencies = y.value_counts()\n","proportions = y.value_counts(normalize=True) * 100\n","print(frequencies)\n","print(proportions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qb33UL3kK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Creating a plot to visualize the distribution of categories\n","sns.countplot(x=y)\n","plt.xlabel('Class')\n","plt.ylabel('Frequency')\n","plt.title('Distribution of Class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wc0q2arzK--n"},"source":["#### **4. Feature Scaling**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48_Yx36mK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["\n","def scale_numerical_features(X_train, X_test, numerical_cols):\n","    \"\"\"\n","    Scales numerical features using StandardScaler.\n","\n","    Parameters:\n","    X_train (pd.DataFrame): Training features.\n","    X_test (pd.DataFrame): Testing features.\n","    numerical_cols (list): List of numerical column names to be scaled.\n","\n","    Returns:\n","    pd.DataFrame: Scaled training features.\n","    pd.DataFrame: Scaled testing features.\n","    \"\"\"\n","    # Initialize MinMaxScaler\n","    scaler = StandardScaler()\n","\n","    # Fit scaler on training data and transform both training and testing data\n","    X_train_scaled = scaler.fit_transform(X_train[numerical_cols])\n","    X_test_scaled = scaler.transform(X_test[numerical_cols])\n","\n","    # Convert scaled arrays back to DataFrames\n","    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=numerical_cols, index=X_train.index)\n","    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=numerical_cols, index=X_test.index)\n","\n","   # Integrate scaled columns back into the original DataFrames\n","    # X_train[numerical_cols] = X_train_scaled_df\n","    # X_test[numerical_cols] = X_test_scaled_df\n","\n","    return X_train_scaled_df, X_test_scaled_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0loxEbp_K--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Ensure that numerical columns are selected for modeling\n","numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale numerical features\n","X_train_scaled_df, X_test_scaled_df = scale_numerical_features(X_train, X_test, numerical_cols)\n","\n","# Verify the scaled features\n","print(\"Scaled training data:\\n\", X_train_scaled_df.head())\n","print(\"Scaled testing data:\\n\", X_test_scaled_df.head())\n"]},{"cell_type":"markdown","metadata":{"id":"PbfzAS_TK--n"},"source":["#### **5. Model Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_xfICpyK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Create an empty DataFrame with an index representing evaluation metrics\n","df_final_test = pd.DataFrame(index=['Precision', 'Recall', 'F1-score', 'Accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"kYwUbbh5K--n"},"source":["##### **Random Forest**"]},{"cell_type":"markdown","metadata":{"id":"HMFkuKdnK--n"},"source":["**Without Features Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJXvBN_aK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Initialize the RandomForestClassifier\n","rf_model = RandomForestClassifier(random_state=42)\n","\n","# Define a refined parameter distribution\n","rf_param_dist_refined = {\n","    'n_estimators': randint(150, 250),\n","    'max_depth': [None, 5, 10, 15],\n","    'min_samples_split': randint(2, 6),\n","    'min_samples_leaf': randint(1, 3),\n","    'bootstrap': [False],\n","    'max_features': ['sqrt', 'log2', None],\n","    'class_weight': [None]\n","}\n","\n","# Re-initialize and run RandomizedSearchCV with refined parameters\n","rf_random_search_refined = RandomizedSearchCV(estimator=rf_model,\n","                                              param_distributions=rf_param_dist_refined,\n","                                              n_iter=100,  # Increase the number of iterations\n","                                              cv=5,\n","                                              n_jobs=-1,\n","                                              verbose=2,\n","                                              random_state=42)\n","rf_random_search_refined.fit(X_train_scaled_df, y_train)\n","\n","# Best parameters and model evaluation\n","print(\"Best parameters for Random Forest (Refined Randomized Search):\", rf_random_search_refined.best_params_)\n","rf_best_model_refined = rf_random_search_refined.best_estimator_\n","y_pred_rf_refined = rf_best_model_refined.predict(X_test_scaled_df)\n","print(\"\\nRandom Forest (Refined Randomized Search) Classification Report:\")\n","print(classification_report(y_test, y_pred_rf_refined))\n","print(\"Random Forest (Refined Randomized Search) Accuracy:\", accuracy_score(y_test, y_pred_rf_refined))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKPTmIZdK--n","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculating evaluation metrics for the Random Forest model tuned on all features\n","accuracy_rf_reduced = accuracy_score(y_test, y_pred_rf_refined)  # Calculating accuracy\n","precision_rf_reduced = precision_score(y_test, y_pred_rf_refined)  # Calculating precision\n","recall_rf_reduced = recall_score(y_test, y_pred_rf_refined)  # Calculating recall\n","f1_rf_reduced = f1_score(y_test, y_pred_rf_refined)  # Calculating F1 score\n","\n","# Printing evaluation metrics\n","print(\"Random Forest Accuracy with All Features (Tuned):\", accuracy_rf_reduced)\n","print(\"Random Forest Precision with All Features (Tuned):\", precision_rf_reduced)\n","print(\"Random Forest Recall with All Features (Tuned):\", recall_rf_reduced)\n","print(\"Random Forest F1 Score with All Features (Tuned):\", f1_rf_reduced)\n","\n","# Appending evaluation metrics to 'df_final_test' for visualization\n","df_final_test[\"Random Forest with All Features (Tuned)\"] = [accuracy_rf_reduced, precision_rf_reduced, recall_rf_reduced, f1_rf_reduced]\n"]},{"cell_type":"markdown","metadata":{"id":"p1GTtVwmK--r"},"source":["**Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtJMMAzFK--r","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Define a more comprehensive parameter grid\n","rf_param_dist_extended = {\n","    'n_estimators': randint(100, 1000),\n","    'max_depth': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n","    'min_samples_split': randint(2, 20),\n","    'min_samples_leaf': randint(1, 10),\n","    'bootstrap': [True, False],\n","    'max_features': ['sqrt', 'log2']\n","}\n","\n","# Initialize and run RandomizedSearchCV with more iterations\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_random_search_extended = RandomizedSearchCV(estimator=rf_model,\n","                                               param_distributions=rf_param_dist_extended,\n","                                               n_iter=200,\n","                                               cv=5,\n","                                               n_jobs=-1,\n","                                               verbose=2,\n","                                               random_state=42)\n","rf_random_search_extended.fit(X_train_scaled_df, y_train)\n","\n","# Best parameters and model evaluation\n","rf_best_model_randomized_extended = rf_random_search_extended.best_estimator_\n","\n","# Feature Importance\n","rf_feature_importances = rf_best_model_randomized_extended.feature_importances_\n","sorted_idx = np.argsort(rf_feature_importances)[::-1]\n","\n","# Select top features\n","X_train_important = X_train_scaled_df.iloc[:, sorted_idx[:10]]\n","X_test_important = X_test_scaled_df.iloc[:, sorted_idx[:10]]\n","\n","# Train and evaluate using top features\n","rf_best_model_randomized_extended.fit(X_train_important, y_train)\n","y_pred_rf = rf_best_model_randomized_extended.predict(X_test_important)\n","\n","print(\"\\nRandom Forest with Important Features Classification Report:\")\n","print(classification_report(y_test, y_pred_rf))\n","print(\"Random Forest with Important Features Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AM9t33itK--r","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":8,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate evaluation metrics for Random Forest with top features\n","accuracy_rf_important = accuracy_score(y_test, y_pred_rf)  # Calculating accuracy\n","precision_rf_important = precision_score(y_test, y_pred_rf)  # Calculating precision\n","recall_rf_important = recall_score(y_test, y_pred_rf)  # Calculating recall\n","f1_rf_important = f1_score(y_test, y_pred_rf)  # Calculating F1 score\n","\n","# Printing evaluation metrics\n","print(\"Random Forest Accuracy with Top Features (Tuned):\", accuracy_rf_important)\n","print(\"Random Forest Precision with Top Features (Tuned):\", precision_rf_important)\n","print(\"Random Forest Recall with Top Features (Tuned):\", recall_rf_important)\n","print(\"Random Forest F1 Score with Top Features (Tuned):\", f1_rf_important)\n","\n","# Appending evaluation metrics to a dataframe for visualization\n","df_final_test[\"Random Forest (Top Features, Tuned)\"] = [accuracy_rf_important, precision_rf_important, recall_rf_important, f1_rf_important]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O18OKXsGK--r","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":8,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Creating a DataFrame for feature importances\n","feature_names = X_train_scaled_df.columns  # Assuming X_train_scaled_df is a DataFrame with column names\n","feature_importances_df = pd.DataFrame({\n","    'Feature': feature_names[sorted_idx],\n","    'Importance': rf_feature_importances[sorted_idx]\n","})\n","\n","print(\"Feature Importances for Random Forest:\\n\", feature_importances_df)"]},{"cell_type":"markdown","metadata":{"id":"6fLl-ypGK--r"},"source":["##### **XGBoost**"]},{"cell_type":"markdown","metadata":{"id":"g1gpdPKNK--r"},"source":["**Without Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gt9h_zIHK--s","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":8,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Convert DataFrames to NumPy arrays\n","X_train_scaled = X_train_scaled_df.values\n","X_test_scaled = X_test_scaled_df.values\n","\n","# Define parameter grid for GridSearchCV\n","param_grid = {\n","    'n_estimators': [50, 100, 200],  # Number of boosting rounds\n","    'max_depth': [3, 6, 10],  # Maximum tree depth for base learners\n","    'learning_rate': [0.01, 0.1, 0.2],  # Boosting learning rate\n","    'subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n","    'colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns when constructing each tree\n","}\n","\n","# Initialize XGBoost model\n","xgb_model = xgb.XGBClassifier(random_state=42)  # Initialize XGBoost model\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)  # Initialize GridSearchCV with 5-fold cross-validation\n","\n","# Train the model using GridSearchCV\n","grid_search.fit(X_train_scaled, y_train)\n","\n","# Display the best parameters\n","print(f\"Best parameters: {grid_search.best_params_}\")  # Print the best parameters found by GridSearchCV\n","print(f\"Best XGBoost Validation Accuracy: {grid_search.best_score_}\")  # Print the best validation accuracy obtained by GridSearchCV\n","\n","# Use the best model for prediction\n","best_xgb_model = grid_search.best_estimator_  # Select the best model found by GridSearchCV\n","y_pred_xgb_tuning = best_xgb_model.predict(X_test_scaled)  # Make predictions using the best model\n","\n","# Model evaluation\n","accuracy = accuracy_score(y_test, y_pred_xgb_tuning)  # Calculate the accuracy\n","print(f\"XGBoost Accuracy: {accuracy}\")  # Print the accuracy\n","print(\"Classification Report:\")  # Print the classification report\n","print(classification_report(y_test, y_pred_xgb_tuning))  # Print the classification report\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fwR5EBuK--s","executionInfo":{"status":"aborted","timestamp":1717603952371,"user_tz":-420,"elapsed":8,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate evaluation metrics for XGBoost with all features tuned\n","accuracy_xgb = accuracy_score(y_test, y_pred_xgb_tuning)  # Calculating accuracy\n","precision_xgb = precision_score(y_test, y_pred_xgb_tuning)  # Calculating precision\n","recall_xgb = recall_score(y_test, y_pred_xgb_tuning)  # Calculating recall\n","f1_xgb = f1_score(y_test, y_pred_xgb_tuning)  # Calculating F1 score\n","\n","# Printing evaluation metrics\n","print(\"XGBoost Accuracy with All Features (Tuned):\", accuracy_xgb)\n","print(\"XGBoost Precision with All Features (Tuned):\", precision_xgb)\n","print(\"XGBoost Recall with All Features (Tuned):\", recall_xgb)\n","print(\"XGBoost F1 Score with All Features (Tuned):\", f1_xgb)\n","\n","# Appending evaluation metrics to DataFrame for visualization\n","df_final_test[\"XGBOOST with All Features (Tuned)\"] = [accuracy_xgb, precision_xgb, recall_xgb, f1_xgb]\n"]},{"cell_type":"markdown","metadata":{"id":"Caz2ps-eK--s"},"source":["**Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utodHfXrK--s","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Train an initial model to get feature importances\n","initial_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n","initial_model.fit(X_train_scaled, y_train)\n","\n","# Get feature importances from the initial model\n","feature_importances = initial_model.feature_importances_\n","feature_names = X_train_scaled_df.columns\n","\n","# Create a DataFrame to hold feature importances\n","importance_xgb_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n","importance_xgb_df = importance_xgb_df.sort_values(by='Importance', ascending=False)\n","\n","# Select top important features\n","xgb_model_top_features = importance_xgb_df.head(10)['Feature'].tolist()\n","print(f\"Top Important Features: {xgb_model_top_features}\")\n","\n","# Define X_train_top_features and X_test_top_features using top important features\n","X_train_top_features = X_train_scaled_df[xgb_model_top_features]\n","X_test_top_features = X_test_scaled_df[xgb_model_top_features]\n","\n","# Define a smaller parameter grid for GridSearchCV\n","param_grid = {\n","    'n_estimators': [50, 100],\n","    'max_depth': [3, 6],\n","    'learning_rate': [0.01, 0.1],\n","    'subsample': [0.8, 1.0],\n","    'colsample_bytree': [0.8, 1.0]\n","}\n","\n","# Initialize XGBoost model\n","xgb_model = xgb.XGBClassifier(random_state=42)\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n","\n","# Train the model using GridSearchCV\n","grid_search.fit(X_train_top_features, y_train)\n","\n","# Display the best parameters and best score\n","print(f\"Best parameters (Top Features): {grid_search.best_params_}\")\n","print(f\"Best XGBoost Validation Accuracy (Top Features): {grid_search.best_score_}\")\n","\n","# Use the best model for prediction\n","best_xgb_model = grid_search.best_estimator_\n","y_pred_xgb = best_xgb_model.predict(X_test_top_features)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred_xgb)\n","print(f\"XGBoost Accuracy with Top Features (Tuned): {accuracy}\")\n","print(\"Classification Report with Top Features (Tuned):\")\n","print(classification_report(y_test, y_pred_xgb))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dA3gS6cSK--s","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate performance metrics for XGBoost with top features\n","accuracy_xgb_top = accuracy_score(y_test, y_pred_xgb)  # Calculating accuracy\n","precision_xgb_top = precision_score(y_test, y_pred_xgb)  # Calculating precision\n","recall_xgb_top = recall_score(y_test, y_pred_xgb)  # Calculating recall\n","f1_xgb_top = f1_score(y_test, y_pred_xgb)  # Calculating F1 score\n","\n","# Print the performance metrics\n","print(\"XGBoost Accuracy with Top Features (Tuned):\", accuracy_xgb_top)\n","print(\"XGBoost Precision with Top Features (Tuned):\", precision_xgb_top)\n","print(\"XGBoost Recall with Top Features (Tuned):\", recall_xgb_top)\n","print(\"XGBoost F1 Score with Top Features (Tuned):\", f1_xgb_top)\n","\n","# Append performance metrics to df_final_test DataFrame\n","df_final_test[\"XGBOOST (Top Features, Tuned)\"] = [accuracy_xgb_top, precision_xgb_top, recall_xgb_top, f1_xgb_top]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hipjnq9uK--s","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Create a DataFrame to hold feature importances\n","importance_xgb_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n","importance_xgb_df = importance_xgb_df.sort_values(by='Importance', ascending=False)\n","\n","# Display the DataFrame with feature importances\n","print(\"Feature Importances DataFrame:\\n\", importance_xgb_df)"]},{"cell_type":"markdown","metadata":{"id":"sWP4O4wKK--s"},"source":["##### **Logistic Regression**"]},{"cell_type":"markdown","metadata":{"id":"wSKx04ciK--s"},"source":["**Without Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLcHJwfbK--s","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Initialize Logistic Regression model\n","logreg_model = LogisticRegression()\n","\n","# Train the model on the training data\n","logreg_model.fit(X_train_scaled_df, y_train)\n","\n","# Predict on the test data\n","y_pred_logreg = logreg_model.predict(X_test_scaled_df)\n","\n","# Define parameter grid for GridSearchCV\n","param_grid_lr = {\n","    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization penalty\n","    'C': [0.01, 0.1, 1, 10],  # Inverse of regularization strength\n","    'solver': ['liblinear', 'saga'],  # Solvers that support L1 and elasticnet regularization\n","    'max_iter': [100, 200, 300],  # Maximum number of iterations\n","    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # Mixing parameter for elasticnet\n","}\n","\n","# Initialize Logistic Regression model\n","lr_model = LogisticRegression(random_state=42)\n","\n","# Filter out invalid combinations of penalty and solver\n","valid_param_grid_lr = []\n","for penalty in param_grid_lr['penalty']:\n","    for solver in param_grid_lr['solver']:\n","        if (penalty == 'elasticnet' and solver != 'saga'):\n","            continue\n","        if penalty == 'elasticnet':\n","            for l1_ratio in param_grid_lr['l1_ratio']:\n","                valid_param_grid_lr.append({'penalty': [penalty], 'C': [C], 'solver': [solver], 'max_iter': [max_iter], 'l1_ratio': [l1_ratio]})\n","        else:\n","            for C in param_grid_lr['C']:\n","                for max_iter in param_grid_lr['max_iter']:\n","                    valid_param_grid_lr.append({'penalty': [penalty], 'C': [C], 'solver': [solver], 'max_iter': [max_iter]})\n","\n","# Initialize GridSearchCV with the valid parameter grid\n","grid_search_lr = GridSearchCV(estimator=lr_model, param_grid=valid_param_grid_lr, cv=5, n_jobs=-1, verbose=2, error_score='raise')\n","\n","# Train the model using GridSearchCV\n","grid_search_lr.fit(X_train_scaled_df, y_train)\n","\n","# Display the best parameters\n","print(f\"Best parameters: {grid_search_lr.best_params_}\")\n","print(f\"Best Logistic Regression Validation Accuracy: {grid_search_lr.best_score_}\")\n","\n","# Use the best model for prediction\n","best_lr_model = grid_search_lr.best_estimator_\n","y_pred_lr_tuning = best_lr_model.predict(X_test_scaled_df)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred_lr_tuning)\n","print(f\"Logistic Regression Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_lr_tuning))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79ROFO-QK--s","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate performance metrics for Logistic Regression with all features tuned\n","accuracy_lr_tuning = accuracy_score(y_test, y_pred_lr_tuning)  # Calculating accuracy\n","precision_lr_tuning = precision_score(y_test, y_pred_lr_tuning)  # Calculating precision\n","recall_lr_tuning = recall_score(y_test, y_pred_lr_tuning)  # Calculating recall\n","f1_lr_tuning = f1_score(y_test, y_pred_lr_tuning)  # Calculating F1 score\n","\n","# Print the performance metrics\n","print(\"Logistic Regression Accuracy with All Features (Tuned):\", accuracy_lr_tuning)\n","print(\"Logistic Regression Precision with All Features (Tuned):\", precision_lr_tuning)\n","print(\"Logistic Regression Recall with All Features (Tuned):\", recall_lr_tuning)\n","print(\"Logistic Regression F1 Score with All Features (Tuned):\", f1_lr_tuning)\n","\n","# Append performance metrics to df_final_test DataFrame\n","df_final_test[\"Logistic Regression with All Features (Tuned)\"] = [accuracy_lr_tuning, precision_lr_tuning, recall_lr_tuning, f1_lr_tuning]\n"]},{"cell_type":"markdown","metadata":{"id":"PQsqFRS-K--s"},"source":["**Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXqw-FM2K--s","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["def get_top_features(X_train_scaled_df, y_train):\n","    # Train an initial model to get feature importances\n","    initial_lr_model = LogisticRegression(random_state=42)\n","    initial_lr_model.fit(X_train_scaled_df, y_train)\n","\n","    # Get feature importances (coefficients) from the initial model\n","    feature_importances_lr = abs(initial_lr_model.coef_[0])\n","    feature_names = X_train_scaled_df.columns\n","\n","    # Create a DataFrame to hold feature importances\n","    importance_lr_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances_lr})\n","\n","    # Sort features by importance\n","    importance_lr_df = importance_lr_df.sort_values(by='Importance', ascending=False)\n","\n","    # Select top important features\n","    lr_model_top_features = importance_lr_df.head(10)['Feature'].tolist()\n","    return lr_model_top_features, importance_lr_df\n","\n","\n","# Example usage:\n","# Assuming X_train_scaled_df and y_train are already defined\n","top_features, importance_lr_df = get_top_features(X_train_scaled_df, y_train)\n","\n","# Display the DataFrame with feature importances\n","print(\"DataFrame with Feature Importances:\")\n","print(importance_lr_df.to_string(index=False))  # Print the DataFrame with feature importances\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GodeSGa6K--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Get top features using the previously defined function\n","top_features, _ = get_top_features(X_train_scaled_df, y_train)\n","\n","# Select only the top features for training and testing datasets\n","X_train_top_features_lr = X_train_scaled_df[top_features]\n","X_test_top_features_lr = X_test_scaled_df[top_features]\n","\n","# Define a smaller parameter grid for GridSearchCV\n","param_grid_lr_top = {\n","    'penalty': ['l1', 'l2'],  # Regularization penalty\n","    'C': [0.01, 0.1, 1, 10],  # Inverse of regularization strength\n","    'solver': ['liblinear', 'saga'],  # Solvers that support L1 regularization\n","    'max_iter': [100, 200, 300]  # Maximum number of iterations\n","}\n","\n","# Initialize Logistic Regression model\n","lr_model_top = LogisticRegression(random_state=42, penalty='none')  # Initialize Logistic Regression model without regularization\n","\n","# Initialize GridSearchCV\n","grid_search_lr_top = GridSearchCV(estimator=lr_model_top, param_grid=param_grid_lr_top, cv=5, n_jobs=-1, verbose=2, error_score='raise')  # Initialize GridSearchCV with 5-fold cross-validation\n","\n","# Train the model using GridSearchCV\n","grid_search_lr_top.fit(X_train_top_features_lr, y_train)\n","\n","# Display the best parameters and best score\n","print(f\"Best parameters (Top Features): {grid_search_lr_top.best_params_}\")  # Print the best parameters found by GridSearchCV\n","print(f\"Best Logistic Regression Validation Accuracy (Top Features): {grid_search_lr_top.best_score_}\")  # Print the best validation accuracy obtained by GridSearchCV\n","\n","# Use the best model for prediction\n","best_lr_model_top = grid_search_lr_top.best_estimator_  # Select the best model found by GridSearchCV\n","y_pred_lr_top = best_lr_model_top.predict(X_test_top_features_lr)  # Make predictions using the best model\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred_lr_top)  # Calculate the accuracy\n","print(f\"Logistic Regression Accuracy with Top Features (Tuned): {accuracy}\")  # Print the accuracy\n","print(\"Classification Report with Top Features (Tuned):\")  # Print the classification report header\n","print(classification_report(y_test, y_pred_lr_top))  # Print the classification report\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewfeiFRQK--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate performance metrics for Logistic Regression with top features tuned\n","accuracy_lr_top_tuning = accuracy_score(y_test, y_pred_lr_top)  # Calculating accuracy\n","precision_lr_top_tuning = precision_score(y_test, y_pred_lr_top)  # Calculating precision\n","recall_lr_top_tuning = recall_score(y_test, y_pred_lr_top)  # Calculating recall\n","f1_lr_top_tuning = f1_score(y_test, y_pred_lr_top)  # Calculating F1 score\n","\n","# Print the performance metrics\n","print(\"Logistic Regression Accuracy with Top Features (Tuned):\", accuracy_lr_top_tuning)\n","print(\"Logistic Regression Precision with Top Features (Tuned):\", precision_lr_top_tuning)\n","print(\"Logistic Regression Recall with Top Features (Tuned):\", recall_lr_top_tuning)\n","print(\"Logistic Regression F1 Score with Top Features (Tuned):\", f1_lr_top_tuning)\n","\n","# Append performance metrics to df_final_test DataFrame\n","df_final_test[\"Logistic Regression (Top Features, Tuned)\"] = [accuracy_lr_top_tuning, precision_lr_top_tuning, recall_lr_top_tuning, f1_lr_top_tuning]\n"]},{"cell_type":"markdown","metadata":{"id":"zaAS4LRrK--t"},"source":["##### **Neural Network**"]},{"cell_type":"markdown","metadata":{"id":"MeFJG722K--t"},"source":["**Without Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysAw8FNNK--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Define a function to create the NN model, required for KerasClassifier\n","def create_model(optimizer='adam', init='uniform'):\n","    model = Sequential()\n","    model.add(Dense(64, input_shape=(X_train_scaled_df.shape[1],), kernel_initializer=init, activation='relu'))  # Input layer with 64 neurons, ReLU activation function\n","    model.add(Dense(32, kernel_initializer=init, activation='relu'))  # Hidden layer with 32 neurons, ReLU activation function\n","    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))  # Output layer with 1 neuron, sigmoid activation function for binary classification\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model with binary crossentropy loss function and accuracy metric\n","    return model\n","\n","# Filter training and testing sets to use all features\n","X_train_all_features_nn = X_train_scaled_df\n","X_test_all_features_nn = X_test_scaled_df\n","\n","# Create the KerasClassifier for GridSearchCV with all features\n","model_all_features = KerasClassifier(\n","    build_fn=create_model,\n","    optimizer='adam',\n","    init='uniform',\n","    verbose=0\n",")\n","\n","# Define the parameter grid for GridSearchCV\n","param_grid = {\n","    'batch_size': [10, 20, 40],  # Batch size for training\n","    'epochs': [10, 50, 100],  # Number of epochs for training\n","    'optimizer': ['adam', 'rmsprop'],  # Optimizers to use\n","    'init': ['uniform', 'normal', 'glorot_uniform']  # Weight initialization methods\n","}\n","\n","# Initialize GridSearchCV for all features\n","grid_search_all_features = GridSearchCV(estimator=model_all_features, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n","\n","# Train the model using GridSearchCV with all features\n","grid_search_all_features.fit(X_train_all_features_nn, y_train)\n","\n","# Display the best parameters and best score\n","print(f\"Best parameters (All Features): {grid_search_all_features.best_params_}\")  # Print the best parameters found by GridSearchCV\n","print(f\"Best Neural Network Validation Accuracy (All Features): {grid_search_all_features.best_score_}\")  # Print the best validation accuracy obtained by GridSearchCV\n","\n","# Use the best model for prediction with all features\n","best_nn_model_all = grid_search_all_features.best_estimator_  # Select the best model found by GridSearchCV\n","y_pred_nn_all = (best_nn_model_all.predict(X_test_all_features_nn) > 0.5).astype(\"int32\")  # Make predictions on the test set using a threshold of 0.5 for binary classification\n","\n","# Evaluate the model with all features\n","accuracy_nn_all = accuracy_score(y_test, y_pred_nn_all)  # Calculate the accuracy\n","print(f\"Neural Network Accuracy with All Features (Tuned): {accuracy_nn_all}\")  # Print the accuracy\n","print(\"Classification Report with All Features (Tuned):\")  # Print the classification report header\n","print(classification_report(y_test, y_pred_nn_all))  # Print the classification report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2suqNXnK--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate performance metrics for Neural Network with all features tuned\n","accuracy_nn_all_tuned = accuracy_score(y_test, y_pred_nn_all)  # Calculating accuracy\n","precision_nn_all_tuned = precision_score(y_test, y_pred_nn_all)  # Calculating precision\n","recall_nn_all_tuned = recall_score(y_test, y_pred_nn_all)  # Calculating recall\n","f1_nn_all_tuned = f1_score(y_test, y_pred_nn_all)  # Calculating F1 score\n","\n","# Print the performance metrics\n","print(\"Neural Network Accuracy with All Features (Tuned):\", accuracy_nn_all_tuned)\n","print(\"Neural Network Precision with All Features (Tuned):\", precision_nn_all_tuned)\n","print(\"Neural Network Recall with All Features (Tuned):\", recall_nn_all_tuned)\n","print(\"Neural Network F1 Score with All Features (Tuned):\", f1_nn_all_tuned)\n","\n","# Append performance metrics to df_final_test DataFrame\n","df_final_test[\"Neural Network (All Features, Tuned)\"] = [accuracy_nn_all_tuned, precision_nn_all_tuned, recall_nn_all_tuned, f1_nn_all_tuned]\n"]},{"cell_type":"markdown","metadata":{"id":"JuTNcanAK--t"},"source":["**Feature Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQ1VTYgQK--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Train an initial model to get feature importances\n","initial_model = Sequential([\n","    Dense(64, input_shape=(X_train_scaled_df.shape[1],), activation='relu'),  # Input layer with 64 neurons, ReLU activation function\n","    Dense(32, activation='relu'),  # Hidden layer with 32 neurons, ReLU activation function\n","    Dense(1, activation='sigmoid')  # Output layer with 1 neuron, sigmoid activation function for binary classification\n","])\n","\n","initial_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model with binary crossentropy loss function and accuracy metric\n","initial_model.fit(X_train_scaled_df, y_train, epochs=10, batch_size=32, verbose=1)  # Train the model with 10 epochs and batch size of 32\n","\n","# Get feature importances from the initial model (using weights from the input layer)\n","feature_importances_nn = abs(initial_model.layers[0].get_weights()[0]).sum(axis=1)  # Extract weights from the input layer and sum them along rows to get feature importances\n","feature_names = X_train_scaled_df.columns  # Get feature names\n","\n","# Create a DataFrame to hold feature importances\n","importance_nn_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances_nn})\n","\n","# Sort features by importance\n","importance_nn_df = importance_nn_df.sort_values(by='Importance', ascending=False)\n","\n","# Select top important features (e.g., top 10)\n","nn_model_top_features = importance_nn_df.head(10)['Feature'].tolist()\n","print(f\"Top Important Features: {nn_model_top_features}\")\n","\n","# Display the DataFrame with feature importances\n","print(importance_nn_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lTMkNh6K--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Define a function to create the NN model, required for KerasClassifier\n","def create_model(optimizer='adam', init='uniform', input_dim=None):\n","    model = Sequential()\n","    model.add(Dense(64, input_shape=(input_dim,), kernel_initializer=init, activation='relu'))  # Input layer with 64 neurons, ReLU activation function\n","    model.add(Dense(32, kernel_initializer=init, activation='relu'))  # Hidden layer with 32 neurons, ReLU activation function\n","    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))  # Output layer with 1 neuron, sigmoid activation function for binary classification\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model with binary crossentropy loss function and accuracy metric\n","    return model\n","\n","# Filter training and testing sets to use only top features\n","X_train_top_features_nn = X_train_scaled_df[nn_model_top_features]\n","X_test_top_features_nn = X_test_scaled_df[nn_model_top_features]\n","\n","# Create the KerasClassifier for GridSearchCV with top features\n","model_top_features = KerasClassifier(\n","    model=create_model,\n","    optimizer='adam',\n","    init='uniform',\n","    input_dim=X_train_top_features_nn.shape[1],\n","    verbose=0\n",")\n","\n","# Define the parameter grid for GridSearchCV\n","param_grid = {\n","    'batch_size': [10, 20, 40],\n","    'epochs': [10, 50, 100],\n","    'optimizer': ['adam', 'rmsprop'],\n","    'init': ['uniform', 'normal', 'glorot_uniform']\n","}\n","\n","# Initialize GridSearchCV for top features\n","grid_search_top_features = GridSearchCV(estimator=model_top_features, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n","\n","# Train the model using GridSearchCV with top features\n","grid_search_top_features.fit(X_train_top_features_nn, y_train)\n","\n","# Display the best parameters and best score\n","print(f\"Best parameters (Top Features): {grid_search_top_features.best_params_}\")\n","print(f\"Best Neural Network Validation Accuracy (Top Features): {grid_search_top_features.best_score_}\")\n","\n","# Use the best model for prediction with top features\n","best_nn_model_top = grid_search_top_features.best_estimator_\n","y_pred_nn_top = (best_nn_model_top.predict(X_test_top_features_nn) > 0.5).astype(\"int32\")\n","\n","# Evaluate the model with top features\n","accuracy_nn_top = accuracy_score(y_test, y_pred_nn_top)\n","print(f\"Neural Network Accuracy with Top Features (Tuned): {accuracy_nn_top}\")\n","print(\"Classification Report with Top Features (Tuned):\")\n","print(classification_report(y_test, y_pred_nn_top))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0gAg01gK--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Calculate performance metrics for Neural Network with top features tuned\n","accuracy_nn_top_tuned = accuracy_score(y_test, y_pred_nn_top)  # Calculating accuracy\n","precision_nn_top_tuned = precision_score(y_test, y_pred_nn_top)  # Calculating precision\n","recall_nn_top_tuned = recall_score(y_test, y_pred_nn_top)  # Calculating recall\n","f1_nn_top_tuned = f1_score(y_test, y_pred_nn_top)  # Calculating F1 score\n","\n","# Print the performance metrics\n","print(\"Neural Network Accuracy with Top Features (Tuned):\", accuracy_nn_top_tuned)\n","print(\"Neural Network Precision with Top Features (Tuned):\", precision_nn_top_tuned)\n","print(\"Neural Network Recall with Top Features (Tuned):\", recall_nn_top_tuned)\n","print(\"Neural Network F1 Score with Top Features (Tuned):\", f1_nn_top_tuned)\n","\n","# Append performance metrics to df_final_test DataFrame\n","df_final_test[\"Neural Network (Top Features, Tuned)\"] = [accuracy_nn_top_tuned, precision_nn_top_tuned, recall_nn_top_tuned, f1_nn_top_tuned]\n"]},{"cell_type":"markdown","metadata":{"id":"8Vdfd14oK--t"},"source":["#### **06. Metric Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0grZ2KC3K--t","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred_xgb_tuning)\n","print(\"Confusion Matrix:\")  # Print the confusion matrix header\n","print(conf_matrix)  # Print the confusion matrix\n","\n","# Generate the confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred_xgb_tuning)\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DBhDaA7KK--u"},"source":["#### **7. Confusion Matrix Per Best Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4VHETEdK--u","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":["# Create a figure with a size of 10x8 inches\n","plt.figure(figsize=(10, 8))\n","\n","# Plot a heatmap of the DataFrame 'df_final_test' with annotations formatted to four decimal places\n","# using the 'coolwarm' colormap, and with annotation text size set to 8\n","sns.heatmap(df_final_test, annot=True, fmt=\".4f\", cmap=\"coolwarm\", annot_kws={\"size\": 8})\n","\n","# Set the title of the plot\n","plt.title(\"Confusion Matrix Per Best Model\", fontsize=16)\n","\n","# Adjust layout to prevent clipping of the title or axis labels\n","plt.tight_layout()\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCX9PNOyK--u","executionInfo":{"status":"aborted","timestamp":1717603952372,"user_tz":-420,"elapsed":9,"user":{"displayName":"MICHELLE VELICE PATRICIA","userId":"12086876002807729668"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}